[{"content":"This page is based on the tutorial posted by @bwaycer\nAccording to Hugo official tutorial and congo tutorial, it is suggested to deploy site through a separate branch called gh-pages. This approach is a bit more complex but has some advantages:\n It keeps sources and generated HTML in two different branches It uses the default public folder It keeps the histories of source branch and gh-pages branch fully separated from each other  Preparations #  Add the public folder to .gitignnore.  echo \u0026quot;public\u0026quot; \u0026gt;\u0026gt; .gitignore Initialize the gh-pages branch as an empty orphan branch.  git checkout --orphan gh-pages git reset --hard git commit --allow-empty -m \u0026quot;Initializing gh-pages branch\u0026quot; git push upstream gh-pages git checkout master Now check out the gh-pages branch into your public folder, using gitâ€™s worktree feature (essentially, it allows you to have multiple branches of the same local repo to be checked out in different directories):  rm -rf public git worktree add -B gh-pages public upstream/gh-pages If you are a Arch-Linux user, like me you probably encounter packages that are not totally up to date from times to times.\nHugo is not an exception, as I write these lines, the package hugo-extended is only available in version 0.83.X and I need the 0.86.1. To do that we are gonna use docker !\nThe docker image # We don\u0026rsquo;t need to make our own, @klakegg made the image klakegg/hugo.\nPull it\ndocker pull klakegg/hugo Run it # Basic command\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:\u0026lt;version\u0026gt; \u0026lt;command\u0026gt;  Exemple 1: I need to run Hugo version 0.86.1 and to execute the command hugo serve -D.\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:0.86.1 serve -D   Exemple 2: I need to run Hugo version 0.83.1 and to execute the command hugo -D.\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:0.83.1 -D Please note that we don\u0026rsquo;t really need to forward any ports to build a hugo website, only to use the included webserver.\n ","date":"11 December 2021","permalink":"/blog/hugo/","section":"Blogs","summary":"This page is based on the tutorial posted by @bwaycer","title":"How to deploy Hugo on Github Page"},{"content":"I\u0026rsquo;m Tengxiao Liu, a second-year MSc student at Fudan University, advised by Prof. Xipeng Qiu. Before joining Fudan, I received my bachelor degree at Xi\u0026rsquo;an Jiaotong University in 2021.\nI am currently working in the area of Natural Language Processing and its applications, especially Question Answering and LLM reasoning.\nEducation #  Fudan University (2021 - present)  Master of Science in Computer Science\n Xi\u0026rsquo;an Jiaotong University (2017 - 2021)  Bachelor of Honors Science Program (Computer Science)\nGPA: 91.92 / 100, Ranking 2 / 32\n Xi\u0026rsquo;an Jiaotong University (2015 - 2017)  Honor\u0026rsquo;s Youth Program\nExperience #  University of California, San Francisco (Feb 2020 - May 2021)  Big Data in Radiology, UCSF\n University of California, Berkeley (Spring 2020)  Berkeley International Study Program, GPA: 4.0/4.0\n University of Alberta (Summer 2019)  Heart disease research using statistical machine learning.\nPublication #  Full Parameter Fine-tuning for Large Language Models with Limited Resources paper  Arxiv, 2023\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu\nwe propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.\n RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees paper  EMNLP, 2022\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang\nWe propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.\n Application of a Domain-specific BERT for Detection of Speech Recognition Errors in Radiology Reports paper  Radiology: Artificiall Intelligence, 2022\nGunvant R. Chaudhari, Tengxiao Liu, Timothy L. Chen, Gabby B. Joseph, Maya Vella, Yoo Jin Lee, Thienkhai H. Vu, Youngho Seo, Andreas M. Rauschecker, Charles E. McCulloch, Jae Ho Sohn\nTo develop radiology domain-specific bidirectional encoder representations from transformers (BERT) models that can identify speech recognition (SR) errors and suggest corrections in radiology reports, we propose Radiology BERT. Our Radiology-specific BERT models fine-tuned on generated errors were able to identify SR errors in radiology reports and suggest corrections\n","date":"1 January 0001","permalink":"/about/","section":"Tengxiao's Page","summary":"I\u0026rsquo;m Tengxiao Liu, a second-year MSc student at Fudan University, advised by Prof.","title":"Greetings! ðŸ‘‹"}]