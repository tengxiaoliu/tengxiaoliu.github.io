[{"content":"This page is based on the tutorial posted by @bwaycer\nAccording to Hugo official tutorial and congo tutorial, it is suggested to deploy site through a separate branch called gh-pages. This approach is a bit more complex but has some advantages:\nIt keeps sources and generated HTML in two different branches It uses the default public folder It keeps the histories of source branch and gh-pages branch fully separated from each other Preparations # Add the public folder to .gitignnore. echo \u0026#34;public\u0026#34; \u0026gt;\u0026gt; .gitignore Initialize the gh-pages branch as an empty orphan branch. git checkout --orphan gh-pages git reset --hard git commit --allow-empty -m \u0026#34;Initializing gh-pages branch\u0026#34; git push upstream gh-pages git checkout master Now check out the gh-pages branch into your public folder, using gitâ€™s worktree feature (essentially, it allows you to have multiple branches of the same local repo to be checked out in different directories): rm -rf public git worktree add -B gh-pages public upstream/gh-pages If you are a Arch-Linux user, like me you probably encounter packages that are not totally up to date from times to times.\nHugo is not an exception, as I write these lines, the package hugo-extended is only available in version 0.83.X and I need the 0.86.1. To do that we are gonna use docker !\nThe docker image # We don\u0026rsquo;t need to make our own, @klakegg made the image klakegg/hugo.\nPull it\ndocker pull klakegg/hugo Run it # Basic command\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:\u0026lt;version\u0026gt; \u0026lt;command\u0026gt; Exemple 1: I need to run Hugo version 0.86.1 and to execute the command hugo serve -D.\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:0.86.1 serve -D Exemple 2: I need to run Hugo version 0.83.1 and to execute the command hugo -D.\ndocker run --rm -it \\ -p 1313:1313 \\ -v $(pwd):/src klakegg/hugo:0.83.1 -D Please note that we don\u0026rsquo;t really need to forward any ports to build a hugo website, only to use the included webserver.\n","date":"December 11, 2021","permalink":"/blog/hugo/","section":"Blogs","summary":"","title":"How to deploy Hugo on Github Page"},{"content":" tengxiao at ucsb dot edu\nMy name in Chinese: åˆ˜è…¾éœ„ I'm Tengxiao Liu, a second-year Ph.D. student at University of California, Santa Barbara, advised by Prof. William Wang. I obtained my Master's at Fudan University, advised by Prof. Xipeng Qiu. I was fortunate to work closely with Prof. Prof. Yue Zhang and Prof. Prof. Zheng Zhang at AWS Shanghai AI Lab. I received my bachelor degree in Computer Science at Xi'an Jiaotong University. I work on natural language processing and generative language models. My current research focuses on:\nEfficient agentic systems for web search, coding, and long-horizon tasks\nPost-training techniques to improve reasoning and reliability in LLMs\nScalable synthetic data generation pipelines for scientific and real-world domains\n[Github] [CV] [Google Scholar] [LinkedIn]\nSelected Publications # Budget-Aware Tool-Use Enables Effective Agent Scaling [paper] [media]\nTengxiao Liu, Zifeng Wang, Jin Miao, I-Hung Hsu, Jun Yan, Jiefeng Chen, Rujun Han, Fangyuan Xu, Yanfei Chen, Ke Jiang, Samira Daruki, Yi Liang, William Yang Wang, Tomas Pfister, Chen-Yu Lee\nPreprint, 2025\nWildSci: Advancing Scientific Reasoning from In-the-Wild Literature [paper]\nTengxiao Liu, Deepak Nathani, Zekun Li, Kevin Yang, William Yang Wang\nNeurIPS AI4Sci Workshop, 2025\nCan Language Models Learn to Skip Steps? [paper]\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Jiayang Cheng, Yue Zhang, Xipeng Qiu, Zheng Zhang\nNeurIPS, 2024\nFull Parameter Fine-tuning for Large Language Models with Limited Resources [paper]\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu\nACL, 2024 (oral)\nPlan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts [paper]\nTengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang\nNeurIPS 2023 MATH-AI Workshop\nEMNLP, 2023\nRLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees [paper]\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang\nEMNLP, 2022\nExperience # Google Cloud AI Research (Summer 2025)\nStudent researcher, advised by Dr. Zifeng Wang\nAmazon AWS AI (Mar 2021 - Aug 2024)\nResearch intern at AWS Shanghai AI Lab, advised by Dr. Qipeng Guo, Prof. Yue Zhang and Prof. Zheng Zhang\nUniversity of California, San Francisco (Feb 2020 - Mar 2021)\nUndergraduate research assistant at Big Data in Radiology, UCSF, advised by Prof. Jae Ho Sohn\n","date":"January 1, 1","permalink":"/about/","section":"","summary":"","title":"About"},{"content":"I\u0026rsquo;m Tengxiao Liu, a third-year MSc student at Fudan University, advised by Prof. Xipeng Qiu. Meanwhile, I\u0026rsquo;m currently a research intern at AWS Shanghai AI Lab, working closely with Prof. Yue Zhang and Prof. Zheng Zhang. Before joining Fudan, I received my bachelor degree at Xi\u0026rsquo;an Jiaotong University in 2021.\nI am currently working in the area of Natural Language Processing and its applications, especially Question Answering and LLM reasoning.\n[Github] [CV]\nEducation # Fudan University (2021 - expected 2024) Master of Science in Computer Science\nSupervisor: Prof. Xipeng Qiu\nXi\u0026rsquo;an Jiaotong University (2017 - 2021) Bachelor of Honors Science Program (Computer Science)\nGPA: 91.92 / 100, Ranking: 2 / 32\nUniversity of California, Berkeley (Spring 2020) GPA: 4.0/4.0, Berkeley International Study Program - Letters \u0026amp; Science (Computer Science)\nXi\u0026rsquo;an Jiaotong University (2015 - 2017) Honor\u0026rsquo;s Youth Program\nExperience # Amazon AWS AI (Mar 2021 - Present) Research intern at AWS Shanghai AI Lab, advised by Dr. Qipeng Guo, Prof. Yue Zhang and Prof. Zheng Zhang\nUniversity of California, San Francisco (Feb 2020 - Mar 2021) Undergraduate research assistant at Big Data in Radiology, UCSF, advised by Jae Ho Sohn, MD, MS\nHuawei (Summer 2020) SDE intern at Smart Wearable and Sports Health PDU\nUniversity of Alberta (Summer 2019) Heart disease research using statistical machine learning, advised by Prof. Linglong Kong\nPublications # Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts paper Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang\nEMNLP, 2023\nWe find that different prompting methods have formed a great complementarity to each other on reasoning tasks. We propose XoT, an automatic problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. We demonstrate the effectiveness of our proposed approach and empirical results suggest that our framework can further generalise to logical reasoning domain.\nFull Parameter Fine-tuning for Large Language Models with Limited Resources paper Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu\nPreprint, 2023\nWe propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.\nRLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees paper Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang\nEMNLP, 2022\nWe propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.\nApplication of a Domain-specific BERT for Detection of Speech Recognition Errors in Radiology Reports paper Gunvant R. Chaudhari, Tengxiao Liu, Timothy L. Chen, Gabby B. Joseph, Maya Vella, Yoo Jin Lee, Thienkhai H. Vu, Youngho Seo, Andreas M. Rauschecker, Charles E. McCulloch, Jae Ho Sohn\nRadiology: Artificiall Intelligence, 2022\nTo develop radiology domain-specific bidirectional encoder representations from transformers (BERT) models that can identify speech recognition (SR) errors and suggest corrections in radiology reports, we propose Radiology BERT. Our Radiology-specific BERT models fine-tuned on generated errors were able to identify SR errors in radiology reports and suggest corrections.\n","date":"January 1, 1","permalink":"/blog/verbose/","section":"Blogs","summary":"","title":"Greetings! ðŸ‘‹"}]