+++
title= "Greetings! :wave:"
type= "about"
+++

I'm Tengxiao Liu, a second-year MSc student at Fudan University, advised by Prof. [Xipeng Qiu](https://xpqiu.github.io). 
Before joining Fudan, I received my bachelor degree at Xi'an Jiaotong University in 2021.

I am currently working in the area of Natural Language Processing and its applications, especially Question Answering and LLM reasoning.

## Education

* **Fudan University** (2021 - present)

Master of Science in Computer Science

* **Xi'an Jiaotong University** (2017 - 2021)

Bachelor of Honors Science Program (Computer Science)

GPA: 91.92 / 100, Ranking 2 / 32

* **Xi'an Jiaotong University** (2015 - 2017)

Honor's Youth Program



## Experience

* **University of California, San Francisco** (Feb 2020 - May 2021)

Big Data in Radiology, UCSF

* **University of California, Berkeley** (Spring 2020)

Berkeley International Study Program, GPA: 4.0/4.0

* **University of Alberta** (Summer 2019)

Heart disease research using statistical machine learning.


## Publication

* **Full Parameter Fine-tuning for Large Language Models with Limited Resources** [paper](https://arxiv.org/pdf/2306.09782.pdf)

Arxiv, 2023

Kai Lv, Yuqing Yang, **Tengxiao Liu**, Qinghui Gao, Qipeng Guo, Xipeng Qiu

We propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.

* **RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees** [paper](https://www.aclanthology.org/2022.emnlp-main.483.pdf)

EMNLP, 2022

**Tengxiao Liu**, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang

We propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework. 

* **Application of a Domain-specific BERT for Detection of Speech Recognition Errors in Radiology Reports** [paper](https://pubmed.ncbi.nlm.nih.gov/35923373/)

Radiology: Artificiall Intelligence, 2022

Gunvant R. Chaudhari, **Tengxiao Liu**, Timothy L. Chen, Gabby B. Joseph, Maya Vella, Yoo Jin Lee, Thienkhai H. Vu, Youngho Seo, Andreas M. Rauschecker, Charles E. McCulloch, Jae Ho Sohn

To develop radiology domain-specific bidirectional encoder representations from transformers (BERT) models that can identify speech recognition (SR) errors and suggest corrections in radiology reports, we propose Radiology BERT. Our Radiology-specific BERT models fine-tuned on generated errors were able to identify SR errors in radiology reports and suggest corrections


